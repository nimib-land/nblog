{
  "data": {"pub_date":{"day":27,"year":2023,"month":3},"path_to_root":".","github_remote_url":"https://github.com/pietroppeter/nblog","path_to_here":"arraymancer_tutorial.nim","highlight":"<link rel='stylesheet' href='https://cdn.jsdelivr.net/gh/pietroppeter/nimib/assets/atom-one-light.css'>","github_logo":"<svg aria-hidden=\"true\" width=\"1.2em\" height=\"1.2em\" style=\"vertical-align: middle;\" preserveAspectRatio=\"xMidYMid meet\" viewBox=\"0 0 16 16\"><path fill-rule=\"evenodd\" d=\"M8 0C3.58 0 0 3.58 0 8c0 3.54 2.29 6.53 5.47 7.59c.4.07.55-.17.55-.38c0-.19-.01-.82-.01-1.49c-2.01.37-2.53-.49-2.69-.94c-.09-.23-.48-.94-.82-1.13c-.28-.15-.68-.52-.01-.53c.63-.01 1.08.58 1.23.82c.72 1.21 1.87.87 2.33.66c.07-.52.28-.87.51-1.07c-1.78-.2-3.64-.89-3.64-3.95c0-.87.31-1.59.82-2.15c-.08-.2-.36-1.02.08-2.12c0 0 .67-.21 2.2.82c.64-.18 1.32-.27 2-.27c.68 0 1.36.09 2 .27c1.53-1.04 2.2-.82 2.2-.82c.44 1.1.16 1.92.08 2.12c.51.56.82 1.27.82 2.15c0 3.07-1.87 3.75-3.65 3.95c.29.25.54.73.54 1.48c0 1.07-.01 1.93-.01 2.2c0 .21.15.46.55.38A8.013 8.013 0 0 0 16 8c0-4.42-3.58-8-8-8z\" fill=\"#000\"></path></svg>","source":"import nimib\nimport nblog\n\nnbInit(theme = useNblog)\nnb.title \"Arraymancer Tutorial - First steps\"\nnb.subtitle \"A remake of the original tutorial using nimib\"\nnb.pubDate 2023, 3, 27\n\nnbText: \"\"\"\n> original tutorial: <https://mratsim.github.io/Arraymancer/tuto.first_steps.html>\n>\n> I will note differences with the original in quoted sections.\n\n## Tensor properties\n\nTensors have the following properties:\n- `rank`: 0 for scalar (cannot be stored), 1 for vector, 2 for matrices, *N* for *N* dimensional arrays\n- `shape`: a sequence of the tensor dimensions along each axis\n\nNext properties are technical and there for completeness:\n- `stride`: a sequence of numbers of steps to get the next item along a dimension\n- `offset`: the first element of the tensor\n\"\"\"\n# order of variable names (d, c, e, ..., a, b) I guess it reflects the original order of the sections.\nnbCode:\n  import arraymancer, sugar, sequtils\n\n  let d = [[1, 2, 3], [4, 5, 6]].toTensor()\n\n  echo d\n\nnbText: \"\"\"> message changed, it was: `Tensor of shape 2x3 of type \"int\" on backend \"Cpu\"`\"\"\"\n\nnbCode:\n  dump d.rank\n  dump d.shape\n  dump d.strides ## [x,y] => next row is x elements away in memory while next column is 1 element away.\n  dump d.offset\nnbText: \"> echo of shape and strides changed (dropped @)\"\n\nnbText: \"\"\"\n## Tensor creation\nThe canonical way to initialize a tensor is by converting a seq of seq of ... or an array of array of ...\ninto a tensor using `toTensor`.\n`toTensor` supports deep nested sequences and arrays, even sequences of array of sequences.\n\"\"\"\n\nnbCode:\n  let c = [\n            [\n              [1,2,3],\n              [4,5,6]\n            ],\n            [\n              [11,22,33],\n              [44,55,66]\n            ],\n            [\n              [111,222,333],\n              [444,555,666]\n            ],\n            [\n              [1111,2222,3333],\n              [4444,5555,6666]\n            ]\n          ].toTensor()\n  echo c\nnbText: \"> I am not sure where the additional pipes come from, maybe a bug?\"\nnbText: \"\"\"\n`newTensor` procedure can be used to initialize a tensor of a specific\nshape with a default value. (0 for numbers, false for bool...)\n\n`zeros` and `ones` procedures create a new tensor filled with 0 and\n1 respectively.\n\n`zeros_like` and `ones_like` take an input tensor and output a\ntensor of the same shape but filled with 0 and 1 respectively.\n\"\"\"\nnbCode:\n  let e = newTensor[bool]([2, 3])\n  dump e\nnbCode:\n  let f = zeros[float]([4, 3])\n  dump f\nnbCode:\n  let g = ones[float]([4, 3])\n  dump g\nnbCode:\n  let tmp = [[1,2],[3,4]].toTensor()\n  let h = tmp.zeros_like\n  dump h\nnbCode:\n  let i = tmp.ones_like\n  dump i\n\nnbText: \"\"\"\n## Accessing and modifying a value\n\nTensors value can be retrieved or set with array brackets.\n\"\"\"\n# need to import sequtils to have toSeq\nnbCode:\n    var a = toSeq(1..24).toTensor().reshape(2,3,4)\n    echo a\nnbCode:\n    dump a[1, 1, 1]\n    echo a\nnbCode:\n    a[1, 1, 1] = 999\n    echo a\nnbText: \"\"\"\n## Copying\n\nWarning ‚ö†: When you do the following, both tensors `a` and `b` will share data.\nFull copy must be explicitly requested via the `clone` function.\n\"\"\"\nblock: # using a block I can reuse a\n  nbCode:\n    let a = toSeq(1..24).toTensor().reshape(2,3,4)\n    var b = a\n    var c = clone(a)\n  nbText: \"\"\"\n  Here modifying `b` WILL modify `a`.\n\n  > adding an example of modification and an example of clone:\n  \"\"\"\n  # still in block scope in order to reuse b\n  nbCode:\n    dump a[1, 0, 0]\n    c[1, 0, 0] = 0\n    dump a[1, 0, 0]\n    b[1, 0, 0] = 0\n    dump a[1, 0, 0]\nnbText: \"\"\"\nThis behaviour is the same as Numpy and Julia,\nreasons can be found in the following\n[under the hood article](https://mratsim.github.io/Arraymancer/uth.copy_semantics.html).\n\"\"\"\n\nnbSaveJson\n","favicon":"<link rel=\"icon\" href=\"data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2280%22>üê≥</text></svg>\">","stylesheet":"<link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/kognise/water.css@latest/dist/light.min.css\">","nb_style":"<style>\n.nb-box {\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n}\n.nb-small {\n  font-size: 0.8rem;\n}\nbutton.nb-small {\n  float: right;\n  padding: 2px;\n  padding-right: 5px;\n  padding-left: 5px;\n}\nsection#source {\n  display:none\n}\n\n.nb-output {\n  line-height: 1.15;\n}\n</style>","title":"Arraymancer Tutorial - First steps - A remake of the original tutorial using nimib","source_highlighted":"<span class=\"hljs-keyword\">import</span> nimib\n<span class=\"hljs-keyword\">import</span> nblog\n\nnbInit(theme = useNblog)\nnb.title <span class=\"hljs-string\">&quot;Arraymancer Tutorial - First steps&quot;</span>\nnb.subtitle <span class=\"hljs-string\">&quot;A remake of the original tutorial using nimib&quot;</span>\nnb.pubDate <span class=\"hljs-number\">2023</span>, <span class=\"hljs-number\">3</span>, <span class=\"hljs-number\">27</span>\n\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n&gt; original tutorial: &lt;https://mratsim.github.io/Arraymancer/tuto.first_steps.html&gt;\n&gt;\n&gt; I will note differences with the original in quoted sections.\n\n## Tensor properties\n\nTensors have the following properties:\n- `rank`: 0 for scalar (cannot be stored), 1 for vector, 2 for matrices, *N* for *N* dimensional arrays\n- `shape`: a sequence of the tensor dimensions along each axis\n\nNext properties are technical and there for completeness:\n- `stride`: a sequence of numbers of steps to get the next item along a dimension\n- `offset`: the first element of the tensor\n&quot;&quot;&quot;</span>\n<span class=\"hljs-comment\"># order of variable names (d, c, e, ..., a, b) I guess it reflects the original order of the sections.</span>\nnbCode:\n  <span class=\"hljs-keyword\">import</span> arraymancer, sugar, sequtils\n\n  <span class=\"hljs-keyword\">let</span> d = [[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>], [<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">5</span>, <span class=\"hljs-number\">6</span>]].toTensor()\n\n  <span class=\"hljs-keyword\">echo</span> d\n\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;&gt; message changed, it was: `Tensor of shape 2x3 of type &quot;int&quot; on backend &quot;Cpu&quot;`&quot;&quot;&quot;</span>\n\nnbCode:\n  dump d.rank\n  dump d.shape\n  dump d.strides <span class=\"hljs-comment\">## [x,y] =&gt; next row is x elements away in memory while next column is 1 element away.</span>\n  dump d.offset\nnbText: <span class=\"hljs-string\">&quot;&gt; echo of shape and strides changed (dropped @)&quot;</span>\n\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n## Tensor creation\nThe canonical way to initialize a tensor is by converting a seq of seq of ... or an array of array of ...\ninto a tensor using `toTensor`.\n`toTensor` supports deep nested sequences and arrays, even sequences of array of sequences.\n&quot;&quot;&quot;</span>\n\nnbCode:\n  <span class=\"hljs-keyword\">let</span> c = [\n            [\n              [<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>],\n              [<span class=\"hljs-number\">4</span>,<span class=\"hljs-number\">5</span>,<span class=\"hljs-number\">6</span>]\n            ],\n            [\n              [<span class=\"hljs-number\">11</span>,<span class=\"hljs-number\">22</span>,<span class=\"hljs-number\">33</span>],\n              [<span class=\"hljs-number\">44</span>,<span class=\"hljs-number\">55</span>,<span class=\"hljs-number\">66</span>]\n            ],\n            [\n              [<span class=\"hljs-number\">111</span>,<span class=\"hljs-number\">222</span>,<span class=\"hljs-number\">333</span>],\n              [<span class=\"hljs-number\">444</span>,<span class=\"hljs-number\">555</span>,<span class=\"hljs-number\">666</span>]\n            ],\n            [\n              [<span class=\"hljs-number\">1111</span>,<span class=\"hljs-number\">2222</span>,<span class=\"hljs-number\">3333</span>],\n              [<span class=\"hljs-number\">4444</span>,<span class=\"hljs-number\">5555</span>,<span class=\"hljs-number\">6666</span>]\n            ]\n          ].toTensor()\n  <span class=\"hljs-keyword\">echo</span> c\nnbText: <span class=\"hljs-string\">&quot;&gt; I am not sure where the additional pipes come from, maybe a bug?&quot;</span>\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n`newTensor` procedure can be used to initialize a tensor of a specific\nshape with a default value. (0 for numbers, false for bool...)\n\n`zeros` and `ones` procedures create a new tensor filled with 0 and\n1 respectively.\n\n`zeros_like` and `ones_like` take an input tensor and output a\ntensor of the same shape but filled with 0 and 1 respectively.\n&quot;&quot;&quot;</span>\nnbCode:\n  <span class=\"hljs-keyword\">let</span> e = newTensor[<span class=\"hljs-built_in\">bool</span>]([<span class=\"hljs-number\">2</span>, <span class=\"hljs-number\">3</span>])\n  dump e\nnbCode:\n  <span class=\"hljs-keyword\">let</span> f = zeros[<span class=\"hljs-built_in\">float</span>]([<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">3</span>])\n  dump f\nnbCode:\n  <span class=\"hljs-keyword\">let</span> g = ones[<span class=\"hljs-built_in\">float</span>]([<span class=\"hljs-number\">4</span>, <span class=\"hljs-number\">3</span>])\n  dump g\nnbCode:\n  <span class=\"hljs-keyword\">let</span> tmp = [[<span class=\"hljs-number\">1</span>,<span class=\"hljs-number\">2</span>],[<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>]].toTensor()\n  <span class=\"hljs-keyword\">let</span> h = tmp.zeros_like\n  dump h\nnbCode:\n  <span class=\"hljs-keyword\">let</span> i = tmp.ones_like\n  dump i\n\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n## Accessing and modifying a value\n\nTensors value can be retrieved or set with array brackets.\n&quot;&quot;&quot;</span>\n<span class=\"hljs-comment\"># need to import sequtils to have toSeq</span>\nnbCode:\n    <span class=\"hljs-keyword\">var</span> a = toSeq(<span class=\"hljs-number\">1.</span>.<span class=\"hljs-number\">24</span>).toTensor().reshape(<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>)\n    <span class=\"hljs-keyword\">echo</span> a\nnbCode:\n    dump a[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>]\n    <span class=\"hljs-keyword\">echo</span> a\nnbCode:\n    a[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">1</span>] = <span class=\"hljs-number\">999</span>\n    <span class=\"hljs-keyword\">echo</span> a\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n## Copying\n\nWarning ‚ö†: When you do the following, both tensors `a` and `b` will share data.\nFull copy must be explicitly requested via the `clone` function.\n&quot;&quot;&quot;</span>\n<span class=\"hljs-keyword\">block</span>: <span class=\"hljs-comment\"># using a block I can reuse a</span>\n  nbCode:\n    <span class=\"hljs-keyword\">let</span> a = toSeq(<span class=\"hljs-number\">1.</span>.<span class=\"hljs-number\">24</span>).toTensor().reshape(<span class=\"hljs-number\">2</span>,<span class=\"hljs-number\">3</span>,<span class=\"hljs-number\">4</span>)\n    <span class=\"hljs-keyword\">var</span> b = a\n    <span class=\"hljs-keyword\">var</span> c = clone(a)\n  nbText: <span class=\"hljs-string\">&quot;&quot;&quot;\n  Here modifying `b` WILL modify `a`.\n\n  &gt; adding an example of modification and an example of clone:\n  &quot;&quot;&quot;</span>\n  <span class=\"hljs-comment\"># still in block scope in order to reuse b</span>\n  nbCode:\n    dump a[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>]\n    c[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>] = <span class=\"hljs-number\">0</span>\n    dump a[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>]\n    b[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>] = <span class=\"hljs-number\">0</span>\n    dump a[<span class=\"hljs-number\">1</span>, <span class=\"hljs-number\">0</span>, <span class=\"hljs-number\">0</span>]\nnbText: <span class=\"hljs-string\">&quot;&quot;&quot;\nThis behaviour is the same as Numpy and Julia,\nreasons can be found in the following\n[under the hood article](https://mratsim.github.io/Arraymancer/uth.copy_semantics.html).\n&quot;&quot;&quot;</span>\n\nnbSaveJson\n"},
  "blocks": [{"command":"nbText","code":"","output":"# Arraymancer Tutorial - First steps","context":{"code":"","output":"# Arraymancer Tutorial - First steps"}},{"command":"nbText","code":"","output":"## A remake of the original tutorial using nimib","context":{"code":"","output":"## A remake of the original tutorial using nimib"}},{"command":"nbRawHtml","code":"","output":"<p><time datetime=\"2023-3-27\">Mar 27, 2023</time></p>","context":{"code":"","output":"<p><time datetime=\"2023-3-27\">Mar 27, 2023</time></p>"}},{"command":"nbText","code":"","output":"> original tutorial: <https://mratsim.github.io/Arraymancer/tuto.first_steps.html>\n>\n> I will note differences with the original in quoted sections.\n\n## Tensor properties\n\nTensors have the following properties:\n- `rank`: 0 for scalar (cannot be stored), 1 for vector, 2 for matrices, *N* for *N* dimensional arrays\n- `shape`: a sequence of the tensor dimensions along each axis\n\nNext properties are technical and there for completeness:\n- `stride`: a sequence of numbers of steps to get the next item along a dimension\n- `offset`: the first element of the tensor\n","context":{"code":"","output":"> original tutorial: <https://mratsim.github.io/Arraymancer/tuto.first_steps.html>\n>\n> I will note differences with the original in quoted sections.\n\n## Tensor properties\n\nTensors have the following properties:\n- `rank`: 0 for scalar (cannot be stored), 1 for vector, 2 for matrices, *N* for *N* dimensional arrays\n- `shape`: a sequence of the tensor dimensions along each axis\n\nNext properties are technical and there for completeness:\n- `stride`: a sequence of numbers of steps to get the next item along a dimension\n- `offset`: the first element of the tensor"}},{"command":"nbCode","code":"import arraymancer, sugar, sequtils\n\nlet d = [[1, 2, 3], [4, 5, 6]].toTensor()\n\necho d","output":"Tensor[system.int] of shape \"[2, 3]\" on backend \"Cpu\"\n|1      2     3|\n|4      5     6|\n","context":{"code":"import arraymancer, sugar, sequtils\n\nlet d = [[1, 2, 3], [4, 5, 6]].toTensor()\n\necho d","output":"Tensor[system.int] of shape \"[2, 3]\" on backend \"Cpu\"\n|1      2     3|\n|4      5     6|"}},{"command":"nbText","code":"","output":"> message changed, it was: `Tensor of shape 2x3 of type \"int\" on backend \"Cpu\"`","context":{"code":"","output":"> message changed, it was: `Tensor of shape 2x3 of type \"int\" on backend \"Cpu\"`"}},{"command":"nbCode","code":"dump d.rank\ndump d.shape\ndump d.strides ## [x,y] => next row is x elements away in memory while next column is 1 element away.\ndump d.offset","output":"d.rank = 2\nd.shape = [2, 3]\nd.strides = [3, 1]\nd.offset = 0\n","context":{"code":"dump d.rank\ndump d.shape\ndump d.strides ## [x,y] => next row is x elements away in memory while next column is 1 element away.\ndump d.offset","output":"d.rank = 2\nd.shape = [2, 3]\nd.strides = [3, 1]\nd.offset = 0"}},{"command":"nbText","code":"","output":"> echo of shape and strides changed (dropped @)","context":{"code":"","output":"> echo of shape and strides changed (dropped @)"}},{"command":"nbText","code":"","output":"## Tensor creation\nThe canonical way to initialize a tensor is by converting a seq of seq of ... or an array of array of ...\ninto a tensor using `toTensor`.\n`toTensor` supports deep nested sequences and arrays, even sequences of array of sequences.\n","context":{"code":"","output":"## Tensor creation\nThe canonical way to initialize a tensor is by converting a seq of seq of ... or an array of array of ...\ninto a tensor using `toTensor`.\n`toTensor` supports deep nested sequences and arrays, even sequences of array of sequences."}},{"command":"nbCode","code":"let c = [\n          [\n            [1,2,3],\n            [4,5,6]\n          ],\n          [\n            [11,22,33],\n            [44,55,66]\n          ],\n          [\n            [111,222,333],\n            [444,555,666]\n          ],\n          [\n            [1111,2222,3333],\n            [4444,5555,6666]\n          ]\n        ].toTensor()\necho c","output":"Tensor[system.int] of shape \"[4, 2, 3]\" on backend \"Cpu\"\n          0                      1                      2                      3           \n|1          2       3| |11        22      33| |111      222     333| |1111    2222    3333|\n|4          5       6| |44        55      66| |444      555     666| |4444    5555    6666|\n\n","context":{"code":"let c = [\n          [\n            [1,2,3],\n            [4,5,6]\n          ],\n          [\n            [11,22,33],\n            [44,55,66]\n          ],\n          [\n            [111,222,333],\n            [444,555,666]\n          ],\n          [\n            [1111,2222,3333],\n            [4444,5555,6666]\n          ]\n        ].toTensor()\necho c","output":"Tensor[system.int] of shape \"[4, 2, 3]\" on backend \"Cpu\"\n          0                      1                      2                      3           \n|1          2       3| |11        22      33| |111      222     333| |1111    2222    3333|\n|4          5       6| |44        55      66| |444      555     666| |4444    5555    6666|"}},{"command":"nbText","code":"","output":"> I am not sure where the additional pipes come from, maybe a bug?","context":{"code":"","output":"> I am not sure where the additional pipes come from, maybe a bug?"}},{"command":"nbText","code":"","output":"`newTensor` procedure can be used to initialize a tensor of a specific\nshape with a default value. (0 for numbers, false for bool...)\n\n`zeros` and `ones` procedures create a new tensor filled with 0 and\n1 respectively.\n\n`zeros_like` and `ones_like` take an input tensor and output a\ntensor of the same shape but filled with 0 and 1 respectively.\n","context":{"code":"","output":"`newTensor` procedure can be used to initialize a tensor of a specific\nshape with a default value. (0 for numbers, false for bool...)\n\n`zeros` and `ones` procedures create a new tensor filled with 0 and\n1 respectively.\n\n`zeros_like` and `ones_like` take an input tensor and output a\ntensor of the same shape but filled with 0 and 1 respectively."}},{"command":"nbCode","code":"let e = newTensor[bool]([2, 3])\ndump e","output":"e = Tensor[system.bool] of shape \"[2, 3]\" on backend \"Cpu\"\n|false    false    false|\n|false    false    false|\n","context":{"code":"let e = newTensor[bool]([2, 3])\ndump e","output":"e = Tensor[system.bool] of shape \"[2, 3]\" on backend \"Cpu\"\n|false    false    false|\n|false    false    false|"}},{"command":"nbCode","code":"let f = zeros[float]([4, 3])\ndump f","output":"f = Tensor[system.float] of shape \"[4, 3]\" on backend \"Cpu\"\n|0      0     0|\n|0      0     0|\n|0      0     0|\n|0      0     0|\n","context":{"code":"let f = zeros[float]([4, 3])\ndump f","output":"f = Tensor[system.float] of shape \"[4, 3]\" on backend \"Cpu\"\n|0      0     0|\n|0      0     0|\n|0      0     0|\n|0      0     0|"}},{"command":"nbCode","code":"let g = ones[float]([4, 3])\ndump g","output":"g = Tensor[system.float] of shape \"[4, 3]\" on backend \"Cpu\"\n|1      1     1|\n|1      1     1|\n|1      1     1|\n|1      1     1|\n","context":{"code":"let g = ones[float]([4, 3])\ndump g","output":"g = Tensor[system.float] of shape \"[4, 3]\" on backend \"Cpu\"\n|1      1     1|\n|1      1     1|\n|1      1     1|\n|1      1     1|"}},{"command":"nbCode","code":"let tmp = [[1,2],[3,4]].toTensor()\nlet h = tmp.zeros_like\ndump h","output":"h = Tensor[system.int] of shape \"[2, 2]\" on backend \"Cpu\"\n|0      0|\n|0      0|\n","context":{"code":"let tmp = [[1,2],[3,4]].toTensor()\nlet h = tmp.zeros_like\ndump h","output":"h = Tensor[system.int] of shape \"[2, 2]\" on backend \"Cpu\"\n|0      0|\n|0      0|"}},{"command":"nbCode","code":"let i = tmp.ones_like\ndump i","output":"i = Tensor[system.int] of shape \"[2, 2]\" on backend \"Cpu\"\n|1      1|\n|1      1|\n","context":{"code":"let i = tmp.ones_like\ndump i","output":"i = Tensor[system.int] of shape \"[2, 2]\" on backend \"Cpu\"\n|1      1|\n|1      1|"}},{"command":"nbText","code":"","output":"## Accessing and modifying a value\n\nTensors value can be retrieved or set with array brackets.\n","context":{"code":"","output":"## Accessing and modifying a value\n\nTensors value can be retrieved or set with array brackets."}},{"command":"nbCode","code":"var a = toSeq(1..24).toTensor().reshape(2,3,4)\necho a","output":"Tensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n          0                      1           \n|1      2     3     4| |13    14    15    16|\n|5      6     7     8| |17    18    19    20|\n|9     10    11    12| |21    22    23    24|\n\n","context":{"code":"var a = toSeq(1..24).toTensor().reshape(2,3,4)\necho a","output":"Tensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n          0                      1           \n|1      2     3     4| |13    14    15    16|\n|5      6     7     8| |17    18    19    20|\n|9     10    11    12| |21    22    23    24|"}},{"command":"nbCode","code":"dump a[1, 1, 1]\necho a","output":"a[1, 1, 1] = 18\nTensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n          0                      1           \n|1      2     3     4| |13    14    15    16|\n|5      6     7     8| |17    18    19    20|\n|9     10    11    12| |21    22    23    24|\n\n","context":{"code":"dump a[1, 1, 1]\necho a","output":"a[1, 1, 1] = 18\nTensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n          0                      1           \n|1      2     3     4| |13    14    15    16|\n|5      6     7     8| |17    18    19    20|\n|9     10    11    12| |21    22    23    24|"}},{"command":"nbCode","code":"a[1, 1, 1] = 999\necho a","output":"Tensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n            0                          1             \n|1        2      3      4| |13      14     15     16|\n|5        6      7      8| |17     999     19     20|\n|9       10     11     12| |21      22     23     24|\n\n","context":{"code":"a[1, 1, 1] = 999\necho a","output":"Tensor[system.int] of shape \"[2, 3, 4]\" on backend \"Cpu\"\n            0                          1             \n|1        2      3      4| |13      14     15     16|\n|5        6      7      8| |17     999     19     20|\n|9       10     11     12| |21      22     23     24|"}},{"command":"nbText","code":"","output":"## Copying\n\nWarning ‚ö†: When you do the following, both tensors `a` and `b` will share data.\nFull copy must be explicitly requested via the `clone` function.\n","context":{"code":"","output":"## Copying\n\nWarning ‚ö†: When you do the following, both tensors `a` and `b` will share data.\nFull copy must be explicitly requested via the `clone` function."}},{"command":"nbCode","code":"let a = toSeq(1..24).toTensor().reshape(2,3,4)\nvar b = a\nvar c = clone(a)","output":"","context":{"code":"let a = toSeq(1..24).toTensor().reshape(2,3,4)\nvar b = a\nvar c = clone(a)","output":""}},{"command":"nbText","code":"","output":"  Here modifying `b` WILL modify `a`.\n\n  > adding an example of modification and an example of clone:\n  ","context":{"code":"","output":"  Here modifying `b` WILL modify `a`.\n\n  > adding an example of modification and an example of clone:\n  "}},{"command":"nbCode","code":"dump a[1, 0, 0]\nc[1, 0, 0] = 0\ndump a[1, 0, 0]\nb[1, 0, 0] = 0\ndump a[1, 0, 0]","output":"a[1, 0, 0] = 13\na[1, 0, 0] = 13\na[1, 0, 0] = 0\n","context":{"code":"dump a[1, 0, 0]\nc[1, 0, 0] = 0\ndump a[1, 0, 0]\nb[1, 0, 0] = 0\ndump a[1, 0, 0]","output":"a[1, 0, 0] = 13\na[1, 0, 0] = 13\na[1, 0, 0] = 0"}},{"command":"nbText","code":"","output":"This behaviour is the same as Numpy and Julia,\nreasons can be found in the following\n[under the hood article](https://mratsim.github.io/Arraymancer/uth.copy_semantics.html).\n","context":{"code":"","output":"This behaviour is the same as Numpy and Julia,\nreasons can be found in the following\n[under the hood article](https://mratsim.github.io/Arraymancer/uth.copy_semantics.html)."}}]
}